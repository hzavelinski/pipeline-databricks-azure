# Projeto de Pipelines de Dados com Airflow e Azure Databricks

Este projeto demonstra a constru√ß√£o de pipelines de dados utilizando Apache Airflow e Azure Databricks para coletar, processar e analisar cota√ß√µes di√°rias de c√¢mbio de uma API. Os dados s√£o transformados e salvos em formato Parquet utilizando PySpark, e s√£o enviados relat√≥rios peri√≥dicos em formato CSV e gr√°ficos com o hist√≥rico das cota√ß√µes.

## üìù Vis√£o Geral do Projeto
### Objetivos
1. **Coletar Dados:** Extrair cota√ß√µes di√°rias de c√¢mbio de uma API p√∫blica e salvar na camada bronze em format parquet.
2. **Processar, Transformar e Armazenar:** Usar Azure Databricks e PySpark para transformar os dados da camada bronze e salv√°-los em formato CSV na camada silver.
3. **Envio de Relat√≥rios:** Enviar tabelas em formato CSV e gr√°ficos com o hist√≥rico das cota√ß√µes no slack.
## üõ†Ô∏è Tecnologias Utilizadas
* Apache Airflow: Para orquestra√ß√£o e agendamento de tarefas.
* Azure Databricks: Para processamento e transforma√ß√£o de dados.
* PySpark: Para manipula√ß√£o e transforma√ß√£o dos dados no Databricks.
* API Layer: Fonte dos dados de c√¢mbio di√°rios.
## üèóÔ∏è Arquitetura do Pipeline
* Extra√ß√£o: Uma tarefa no Airflow coleta cota√ß√µes di√°rias de c√¢mbio da API Layer.
* Transforma√ß√£o: Os dados brutos s√£o enviados para o Azure Databricks onde s√£o transformados utilizando PySpark e salvos em formato Parquet.
* Carregamento: Os dados transformados s√£o armazenados em um Data Lake ou Data Warehouse.
* Relat√≥rios: Gerar gr√°ficos e tabelas em CSV, e envi√°-los como parte dos relat√≥rios.
